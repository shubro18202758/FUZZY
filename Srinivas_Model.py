# -*- coding: utf-8 -*-
"""UPI Fraud Detection(FAM)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzfWdeNV3kzcI-MiSubkSCIATx2mV0Sq
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load the dataset
df = pd.read_csv('upi_fraud_dataset.csv')

# Display first few rows of the dataframe
print(df.head())

# Drop irrelevant columns (Id, upi_number)
df.drop(columns=['Id', 'upi_number'], inplace=True)

# Convert categorical variables to numerical (e.g., category, state)
label_encoders = {}
for col in ['category', 'state']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Normalize numerical features
scaler = StandardScaler()
numerical_features = ['trans_hour', 'trans_day', 'trans_month', 'trans_year', 'age', 'trans_amount', 'zip']
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Define input features and target
X = df.drop(columns=['fraud_risk']).values
y = df['fraud_risk'].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

""" Implement AMDV-ART Algorithm

"""

class AMDV_ART:
    def __init__(self, alpha=0.001, beta=0.5, rho_lo=0.7, rho_hi=0.95, K=10):
        self.alpha = alpha  # Choice parameter
        self.beta = beta    # Learning rate
        self.rho_lo = rho_lo  # Lower vigilance
        self.rho_hi = rho_hi  # Upper vigilance
        self.K = K          # Meta-update interval
        self.categories = []  # List of weight vectors
        self.map = {}       # Mapping from category index to class label

    def complement_code(self, x):
        return np.concatenate((x, 1 - x), axis=0)

    def match(self, x, w):
        return np.sum(np.minimum(x, w)) / np.sum(x)

    def fit(self, X, y):
        num_samples, dim = X.shape
        X_cc = np.apply_along_axis(self.complement_code, 1, X)  # Complement-coded inputs

        meta_step = 0
        for idx in range(num_samples):
            x = X_cc[idx]
            y_true = y[idx]

            # Compute choice function
            T = []
            for j, w in enumerate(self.categories):
                Tj = np.sum(np.minimum(x, w)) / (self.alpha + np.sum(w))
                T.append((Tj, j))

            if T:
                T.sort(reverse=True)
                best_T, best_j = T[0]
                match_score = self.match(x, self.categories[best_j])

                if match_score >= self.rho_lo:
                    predicted_class = self.map.get(best_j, None)
                    if predicted_class == y_true:
                        # Update weights
                        self.categories[best_j] = self.beta * np.minimum(x, self.categories[best_j]) + \
                                                  (1 - self.beta) * self.categories[best_j]
                    else:
                        # Micro match tracking: increase vigilance slightly
                        self.rho_lo = min(self.rho_lo + 0.01, self.rho_hi)
                        continue
                else:
                    # Create new category node
                    self.categories.append(x.copy())
                    self.map[len(self.categories) - 1] = y_true
            else:
                # First category
                self.categories.append(x.copy())
                self.map[0] = y_true

            # Every K samples, perform meta-update
            meta_step += 1
            if meta_step % self.K == 0:
                self.meta_update(X_cc[:idx+1], y[:idx+1])

    def meta_update(self, X, y):
        # Placeholder for quasi-Newton update logic
        pass

    def predict(self, X):
        X_cc = np.apply_along_axis(self.complement_code, 1, X)
        predictions = []
        for x in X_cc:
            T = []
            for j, w in enumerate(self.categories):
                Tj = np.sum(np.minimum(x, w)) / (self.alpha + np.sum(w))
                T.append((Tj, j))
            T.sort(reverse=True)
            if T:
                _, best_j = T[0]
                predictions.append(self.map.get(best_j, 0))
            else:
                predictions.append(0)
        return np.array(predictions)

    def score(self, X, y):
        y_pred = self.predict(X)
        accuracy = np.mean(y_pred == y)
        return accuracy

""" Train and Evaluate the Model"""

# Initialize and train the AMDV-ART model
model = AMDV_ART(alpha=0.001, beta=0.5, rho_lo=0.7, rho_hi=0.95, K=10)
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Evaluate model performance
from sklearn.metrics import classification_report, accuracy_score

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

""" Prediction Example"""

class ARTMAPEnsemble:
    def __init__(self, n_models=5, art_params=None):
        if art_params is None:
            art_params = {}
        self.models = [AMDV_ART(**art_params) for _ in range(n_models)]

    def fit(self, X, y):
        for model in self.models:
            idx = np.random.permutation(len(X))
            model.fit(X[idx], y[idx])

    def predict(self, X):
        all_predictions = np.array([model.predict(X) for model in self.models])
        # Majority voting
        final_predictions = []
        for i in range(X.shape[0]):
            votes = all_predictions[:, i]
            most_common = np.bincount(votes).argmax()
            final_predictions.append(most_common)
        return np.array(final_predictions)

    def score(self, X, y):
        y_pred = self.predict(X)
        return accuracy_score(y, y_pred)

# Example usage
# Initialize ensemble
ensemble = ARTMAPEnsemble(n_models=5, art_params={"alpha": 0.001, "beta": 0.5, "rho_lo": 0.7, "rho_hi": 0.95, "K": 10})
ensemble.fit(X_train, y_train)

# Predict and evaluate
y_pred_ensemble = ensemble.predict(X_test)
print("Ensemble Accuracy:", accuracy_score(y_test, y_pred_ensemble))
print(classification_report(y_test, y_pred_ensemble))

# Example prediction
sample_input = np.array([[
    1, 1, 1, 2022, 12, 54, 66.21, 22, 49879  # Example values (replace with real data)
]])

# Normalize and encode the sample input
sample_input_df = pd.DataFrame(sample_input, columns=[
    'trans_hour', 'trans_day', 'trans_month', 'trans_year', 'category',
    'age', 'trans_amount', 'state', 'zip'
])

# Apply same preprocessing
for col in ['category', 'state']:
    sample_input_df[col] = label_encoders[col].transform(sample_input_df[col])

sample_input_df[numerical_features] = scaler.transform(sample_input_df[numerical_features])
sample_processed = sample_input_df.values

# Make prediction
prediction = model.predict(sample_processed)
print("Fraud Risk Prediction:", "Yes" if prediction[0] == 1 else "No")